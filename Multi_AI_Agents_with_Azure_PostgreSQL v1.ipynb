{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --quiet autogen langchain langchain_openai langchain-community langchain_experimental gradio psycopg2 python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "from langchain_community.utilities.sql_database import SQLDatabase\n",
    "from langchain_experimental.sql.base import SQLDatabaseChain\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "import asyncio\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from the .env file from the same directory as notebook \n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve environment variables\n",
    "POSTGRES_USER = os.getenv('POSTGRES_USER')\n",
    "POSTGRES_PASSWORD = os.getenv('POSTGRES_PASSWORD')\n",
    "POSTGRES_HOST = os.getenv('POSTGRES_HOST')\n",
    "POSTGRES_PORT = os.getenv('POSTGRES_PORT')\n",
    "POSTGRES_DB = os.getenv('POSTGRES_DB')\n",
    "AZURE_OPENAI_KEY = os.getenv('AZURE_OPENAI_KEY')\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "AZURE_OPENAI_DEPLOYMENT = os.getenv('AZURE_OPENAI_DEPLOYMENT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the database URI\n",
    "shipment_db_uri = f\"postgresql+psycopg2://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}\"\n",
    "crm_db_uri = f\"postgresql+psycopg2://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}\"\n",
    "\n",
    "\n",
    "# Establish database connections\n",
    "shipment_db = SQLDatabase.from_uri(shipment_db_uri)\n",
    "crm_db = SQLDatabase.from_uri(crm_db_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Azure OpenAI language model\n",
    "azure_llm = AzureChatOpenAI(\n",
    "  azure_endpoint = AZURE_OPENAI_ENDPOINT,\n",
    "  api_key=AZURE_OPENAI_KEY,\n",
    "  api_version=\"2024-10-21\",\n",
    "  deployment_name=AZURE_OPENAI_DEPLOYMENT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query functions for each database\n",
    "def query_shipment(query):\n",
    "    return shipment_chain.invoke(query)\n",
    "\n",
    "def query_crm(query):\n",
    "    return crm_chain.invoke(query)\n",
    "\n",
    "# Function to retrieve database schema information\n",
    "def get_schema_info():\n",
    "    from sqlalchemy import text\n",
    "    with shipment_db._engine.connect() as connection:\n",
    "        query = text(\"\"\"\n",
    "        SELECT\n",
    "            cols.table_schema,\n",
    "            cols.table_name,\n",
    "            cols.column_name,\n",
    "            cols.data_type,\n",
    "            cols.is_nullable,\n",
    "            cons.constraint_type,\n",
    "            cons.constraint_name,\n",
    "            fk.references_table AS referenced_table,\n",
    "            fk.references_column AS referenced_column\n",
    "        FROM information_schema.columns cols\n",
    "        LEFT JOIN information_schema.key_column_usage kcu\n",
    "            ON cols.table_schema = kcu.table_schema\n",
    "            AND cols.table_name = kcu.table_name\n",
    "            AND cols.column_name = kcu.column_name\n",
    "        LEFT JOIN information_schema.table_constraints cons\n",
    "            ON kcu.table_schema = cons.table_schema\n",
    "            AND kcu.table_name = cons.table_name\n",
    "            AND kcu.constraint_name = cons.constraint_name\n",
    "        LEFT JOIN (\n",
    "            SELECT\n",
    "                rc.constraint_name,\n",
    "                kcu.table_name AS references_table,\n",
    "                kcu.column_name AS references_column\n",
    "            FROM information_schema.referential_constraints rc\n",
    "            JOIN information_schema.key_column_usage kcu\n",
    "                ON rc.unique_constraint_name = kcu.constraint_name\n",
    "        ) fk\n",
    "            ON cons.constraint_name = fk.constraint_name\n",
    "        WHERE cols.table_schema = 'public'\n",
    "        ORDER BY cols.table_schema, cols.table_name, cols.ordinal_position;\n",
    "        \"\"\")\n",
    "        result = connection.execute(query)\n",
    "        columns = result.keys()\n",
    "        rows = result.fetchall()\n",
    "        # Convert the result to a list of dictionaries\n",
    "        schema_info = [dict(zip(columns, row)) for row in rows]\n",
    "    return schema_info\n",
    "\n",
    "# Function to share schema information between agents\n",
    "def get_shared_schema_info():\n",
    "    if schema_agent.schema_info is None:\n",
    "        schema_agent.retrieve_and_store_schema()\n",
    "    return schema_agent.schema_info\n",
    "\n",
    "# Method to retrieve and store schema information\n",
    "def retrieve_and_store_schema(agent):\n",
    "    schema_info = get_schema_info()\n",
    "    agent.schema_info = schema_info\n",
    "    return \"Schema information retrieved and stored.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language model configuration with functions\n",
    "llm_config = {\n",
    "    \"config_list\": [\n",
    "        {\n",
    "        \"model\": AZURE_OPENAI_DEPLOYMENT,\n",
    "        \"temperature\": 0.7,\n",
    "        \"api_key\": AZURE_OPENAI_KEY,\n",
    "        \"azure_endpoint\": AZURE_OPENAI_ENDPOINT,\n",
    "        \"api_type\": \"azure\",\n",
    "        \"api_version\": \"2024-10-21\"\n",
    "        }\n",
    "    ],\n",
    "    \"seed\": 42,\n",
    "    \"functions\": [\n",
    "        {\n",
    "            \"name\": \"query_shipment\",\n",
    "            \"description\": \"Queries the Shipment database based on the provided query\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\"type\": \"string\", \"description\": \"The SQL query to execute on the shipment database\"}\n",
    "                },\n",
    "                \"required\": [\"query\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"query_crm\",\n",
    "            \"description\": \"Queries the CRM database based on the provided query\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\"type\": \"string\", \"description\": \"The SQL query to execute on the CRM database\"}\n",
    "                },\n",
    "                \"required\": [\"query\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"get_schema_info\",\n",
    "            \"description\": \"Retrieves the database schema and referential integrity information.\",\n",
    "            \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"get_shared_schema_info\",\n",
    "            \"description\": \"Provides the stored schema information to other agents.\",\n",
    "            \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Initialize the database chains\n",
    "shipment_chain = SQLDatabaseChain(llm=azure_llm, database=shipment_db, verbose=True)\n",
    "crm_chain = SQLDatabaseChain(llm=azure_llm, database=crm_db, verbose=True)\n",
    "\n",
    "# Create assistant agents\n",
    "shipment_agent = autogen.AssistantAgent(\n",
    "    name=\"ShipmentAgent\",\n",
    "    llm_config=llm_config,\n",
    "    description=\"Handles shipments in the main database.\",\n",
    "    system_message=(\n",
    "        \"Your role is to query the main database using 'query_shipment'. \"\n",
    "        \"Focus on the shipments tables and ensure that all shipments are tracked correctly. You can make SELECT, INSERT, DELETE, UPDATE using PostgreSQL queries. Before making Update, Insert, Delete, confirm with user_proxy agents.\"\n",
    "        \"Use 'get_shared_schema_info' to retrieve schema information.\"\n",
    "        \"If the user's question has already been answered, do not provide redundant information.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "crm_agent = autogen.AssistantAgent(\n",
    "    name=\"CRMAgent\",\n",
    "    llm_config=llm_config,\n",
    "    description=\"Manages customer and product information in the second database.\",\n",
    "    system_message=(\n",
    "        \"Your role is to query the second database using 'query_crm'. \"\n",
    "        \"Focus on maintaining the customers and product tables. You can make SELECT, INSERT, DELETE, UPDATE using PostgreSQL queries. Before making Update, Insert, Delete, confirm with user_proxy agents.\"\n",
    "        \"Use 'get_shared_schema_info' to retrieve schema information.\"\n",
    "        \"If the user's question has already been answered, do not provide redundant information.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "schema_agent = autogen.AssistantAgent(\n",
    "    name=\"SchemaAgent\",\n",
    "    llm_config=llm_config,\n",
    "    description=\"Understands and shares database schema information.\",\n",
    "    system_message=(\n",
    "        \"Your role is to retrieve and understand the database schema and referential integrity constraints. \"\n",
    "        \"Use 'get_schema_info' to retrieve schema information and store it. \"\n",
    "        \"If the user's question has already been answered, do not provide redundant information.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Register functions with the agents\n",
    "shipment_agent.register_function(function_map={\"query_shipment\": query_shipment})\n",
    "crm_agent.register_function(function_map={\"query_crm\": query_crm})\n",
    "schema_agent.register_function(\n",
    "    function_map={\n",
    "        \"get_schema_info\": get_schema_info,\n",
    "        \"get_shared_schema_info\": get_shared_schema_info,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add schema_info attribute and bind method to schema_agent\n",
    "import types\n",
    "schema_agent.schema_info = None\n",
    "schema_agent.retrieve_and_store_schema = types.MethodType(retrieve_and_store_schema, schema_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a user proxy agent\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"User_proxy\",\n",
    "    system_message=\"A human admin.\",\n",
    "    code_execution_config={\n",
    "        \"last_n_messages\": 4,\n",
    "        \"work_dir\": \"groupchat\",\n",
    "        \"use_docker\": False,\n",
    "    },\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "# Set up the group chat and manager\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[user_proxy, schema_agent, shipment_agent, crm_agent],\n",
    "    messages=[],\n",
    "    max_round=20 # Maximum number of rounds in the conversation\n",
    ")\n",
    "\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "# Initialize chat history\n",
    "chat_history = []\n",
    "\n",
    "async def process_user_input(user_message, chat_history):\n",
    "    # Append your message with role 'assistant' (to appear on the left)\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": user_message})\n",
    "\n",
    "    # Append a placeholder for the agent's response with role 'user' (to appear on the right)\n",
    "    placeholder_index = len(chat_history)\n",
    "    chat_history.append({\"role\": \"user\", \"content\": \"Processing...\"})\n",
    "\n",
    "    # Return the updated chat history immediately\n",
    "    yield chat_history, chat_history\n",
    "\n",
    "    # Now process the agent's response\n",
    "    # Use the user_proxy agent to process the message\n",
    "    await asyncio.to_thread(user_proxy.initiate_chat, manager, message=user_message)\n",
    "\n",
    "    # Collect messages from the agents\n",
    "    agent_messages = [\n",
    "        msg for msg in manager.groupchat.messages if msg.get(\"role\", \"\") != \"System\"\n",
    "    ]\n",
    "\n",
    "    # Remove the placeholder\n",
    "    if placeholder_index < len(chat_history):\n",
    "        chat_history.pop(placeholder_index)\n",
    "\n",
    "    # Append each agent's message to the chat history\n",
    "    for msg in agent_messages:\n",
    "        name = msg.get(\"name\", \"Agent\")\n",
    "        content = msg.get(\"content\", \"\")\n",
    "        role = \"user\"  # Agents' messages will appear on the right\n",
    "\n",
    "        # Include the agent's name in the content\n",
    "        content_with_name = f\"**{name}**: {content}\"\n",
    "\n",
    "        # Append to chat history\n",
    "        chat_history.append({\"role\": role, \"content\": content_with_name})\n",
    "\n",
    "        # Yield after each agent's message to update the UI\n",
    "        yield chat_history, chat_history\n",
    "\n",
    "    # Return the final chat history\n",
    "    yield chat_history, chat_history\n",
    "\n",
    "def gradio_chat_interface():\n",
    "    with gr.Blocks() as demo:\n",
    "        chat_history_state = gr.State([])\n",
    "\n",
    "        gr.Markdown(\"# Multi-Agent Chat Interface\")\n",
    "\n",
    "        with gr.Row():\n",
    "            chatbot = gr.Chatbot(type=\"messages\")  # Use 'messages' format\n",
    "        with gr.Row():\n",
    "            user_input = gr.Textbox(\n",
    "                placeholder=\"Type your message here...\",\n",
    "                show_label=False\n",
    "            )\n",
    "            send_button = gr.Button(\"Send\")\n",
    "            clear_button = gr.Button(\"Clear Chat\")\n",
    "\n",
    "        async def on_user_message(user_message, chat_history):\n",
    "            if user_message:\n",
    "                # Process the user input and get the updated chat history\n",
    "                # Use a generator to handle incremental updates\n",
    "                response = process_user_input(user_message, chat_history)\n",
    "                async for chat_history_update, chat_history_state_update in response:\n",
    "                    await asyncio.sleep(0)\n",
    "                    yield gr.update(value=chat_history_update), chat_history_state_update\n",
    "\n",
    "        send_button.click(\n",
    "            on_user_message,\n",
    "            inputs=[user_input, chat_history_state],\n",
    "            outputs=[chatbot, chat_history_state]\n",
    "        )\n",
    "\n",
    "        user_input.submit(\n",
    "            on_user_message,\n",
    "            inputs=[user_input, chat_history_state],\n",
    "            outputs=[chatbot, chat_history_state]\n",
    "        )\n",
    "\n",
    "        clear_button.click(\n",
    "            lambda: ([], []),\n",
    "            None,\n",
    "            [chatbot, chat_history_state],\n",
    "            queue=False\n",
    "        )\n",
    "\n",
    "    return demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_proxy.initiate_chat(manager, message=\"Which products with names are currently tracking in transit?\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_proxy.initiate_chat(\n",
    "    manager,\n",
    "    message=(\n",
    "        \"Is Marc a Customer?\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_proxy.initiate_chat(\n",
    "    manager,\n",
    "    message=(\n",
    "        \"Can you add MarcR with email address marc@contoso.com, phone number +1 123 456 7890 and address in 1 Main Street, Bellevue?\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_proxy.initiate_chat(\n",
    "    manager,\n",
    "    message=(\n",
    "        \"Can you create a new shipment of 1 Laptop and 1 Smartphone to MarcR and ensure shipment is updated to Departed Origin from the location in New York and towards Los Angeles date is today?\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Gradio interface\n",
    "demo = gradio_chat_interface()\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
